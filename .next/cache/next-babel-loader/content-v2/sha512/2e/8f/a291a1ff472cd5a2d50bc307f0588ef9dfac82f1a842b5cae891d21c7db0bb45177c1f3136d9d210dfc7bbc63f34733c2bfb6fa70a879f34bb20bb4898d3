{"ast":null,"code":"const OpenAI = require('openai-api');\n\nconst openai = new OpenAI(process.env.REACT_APP_API_URL);\nexport default (async (req, res) => {\n  let prompt = `Question: ${req.body.name}\\n\\nAnswer:\\n`;\n  const gptResponse = await openai.complete({\n    engine: 'curie:ft-personal-2022-10-02-01-41-08',\n    prompt: prompt,\n    maxTokens: 256,\n    temperature: 0.6,\n    topP: 1,\n    presencePenalty: 0,\n    frequencyPenalty: 0.1,\n    bestOf: 1,\n    n: 1,\n    stop: [\"\\\\n\", \"END\"]\n  });\n  res.status(200).json({\n    text: `${gptResponse.data.choices[0].text}`\n  });\n});","map":{"version":3,"sources":["/Users/ayushbaweja/Development/DocuBot/pages/api/openai.js"],"names":["OpenAI","require","openai","process","env","REACT_APP_API_URL","req","res","prompt","body","name","gptResponse","complete","engine","maxTokens","temperature","topP","presencePenalty","frequencyPenalty","bestOf","n","stop","status","json","text","data","choices"],"mappings":"AAAA,MAAMA,MAAM,GAAGC,OAAO,CAAC,YAAD,CAAtB;;AACA,MAAMC,MAAM,GAAG,IAAIF,MAAJ,CAAWG,OAAO,CAACC,GAAR,CAAYC,iBAAvB,CAAf;AAIA,gBAAe,OAAOC,GAAP,EAAYC,GAAZ,KAAoB;AACjC,MAAIC,MAAM,GAAI,aAAYF,GAAG,CAACG,IAAJ,CAASC,IAAK,eAAxC;AACA,QAAMC,WAAW,GAAG,MAAMT,MAAM,CAACU,QAAP,CAAgB;AACxCC,IAAAA,MAAM,EAAE,uCADgC;AAExCL,IAAAA,MAAM,EAAEA,MAFgC;AAGxCM,IAAAA,SAAS,EAAE,GAH6B;AAIxCC,IAAAA,WAAW,EAAE,GAJ2B;AAKxCC,IAAAA,IAAI,EAAE,CALkC;AAMxCC,IAAAA,eAAe,EAAE,CANuB;AAOxCC,IAAAA,gBAAgB,EAAE,GAPsB;AAQxCC,IAAAA,MAAM,EAAE,CARgC;AASxCC,IAAAA,CAAC,EAAE,CATqC;AAUxCC,IAAAA,IAAI,EAAE,CAAC,KAAD,EAAQ,KAAR;AAVkC,GAAhB,CAA1B;AAaAd,EAAAA,GAAG,CAACe,MAAJ,CAAW,GAAX,EAAgBC,IAAhB,CAAqB;AAACC,IAAAA,IAAI,EAAG,GAAEb,WAAW,CAACc,IAAZ,CAAiBC,OAAjB,CAAyB,CAAzB,EAA4BF,IAAK;AAA3C,GAArB;AACD,CAhBD","sourcesContent":["const OpenAI = require('openai-api');\nconst openai = new OpenAI(process.env.REACT_APP_API_URL);\n\n\n\nexport default async (req, res) => {\n  let prompt = `Question: ${req.body.name}\\n\\nAnswer:\\n`;\n  const gptResponse = await openai.complete({\n    engine: 'curie:ft-personal-2022-10-02-01-41-08',\n    prompt: prompt,\n    maxTokens: 256,\n    temperature: 0.6,\n    topP: 1,\n    presencePenalty: 0,\n    frequencyPenalty: 0.1,\n    bestOf: 1,\n    n: 1,\n    stop: [\"\\\\n\", \"END\"]\n});\n  \n  res.status(200).json({text: `${gptResponse.data.choices[0].text}`})\n}\n"]},"metadata":{},"sourceType":"module"}